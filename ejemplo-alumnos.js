import { Ollama } from "@llamaindex/ollama";
import { Settings } from "llamaindex";
import readline from "readline";

// Configura el modelo Ollama
// AsegÃºrate de tener el modelo descargado y corriendo en tu mÃ¡quina
// Baja ollama  de https://ollama.com/ 
// corre el siguiente comando en la terminal: `ollama run gemma3:1b`
const ollamaLLM = new Ollama({
  model: "gemma3:1b", // Cambia el modelo si lo deseas por ejemplo : "mistral:7b", "llama2:7b", etc.
  temperature: 0.75,
});

const HistorialConversacion = [
  {
    role: "system",
    content: `Sos un orientador vocacional amable y empÃ¡tico. 
    Tu objetivo es hacer preguntas sobre gustos e intereses 
    y sugerir trayectorias educativas o laborales en funciÃ³n de las respuestas. 
    Usa un lenguaje amable, claro y accesible para cualquier usuario.`
  }
];

const preguntasDefault = [
  "Â¿CuÃ¡les son tus actividades extracurriculares favoritas y tus materias favoritas del colegio?",
  "Â¿Te gustan los actividades mÃ¡s prÃ¡cticos y tÃ©cnicas o preferÃ­s lo que son mÃ¡s abstractas y creativas?",
  "Â¿CuÃ¡l es un tema del que no conoces mucho y te gustarÃ­a aprender mÃ¡s sobre el?",
];

let preguntaIndex = 0;

// Asigna Ollama como LLM y modelo de embeddings
Settings.llm = ollamaLLM;
Settings.embedModel = ollamaLLM;

// FunciÃ³n principal
async function main() {
  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
  });

  console.log("ðŸŽ“ Bot Vocacional iniciado.");
  console.log("Voy a hacer un par de preguntas para conocerte y sugerirte trayectorias educativas o laborales en funciÃ³n de tus respuestas.");
  console.log("EscribÃ­ 'salir' si querÃ©s terminar.\n");

  console.log("ðŸ¤– IA:", preguntas[preguntaIndex]);

  rl.on("line", async (input) => {
    if (input.toLowerCase() === "salir") {
      rl.close();
      return;
    }

    conversationHistory.push({ role: "user", content: input });

    preguntaIndex++;

    if (preguntaIndex < preguntas.length) {
      console.log("ðŸ¤– IA:", preguntas[preguntaIndex]);
    } else {
      try {
        conversationHistory.push({
          role: "system",
          content: "Con toda la informaciÃ³n recolectada sugerile al menos 2 posibles carreras o trayectorias educativas adecuadas para el usuario y justifica el motivo."
        });

        const res = await ollamaLLM.chat({
          messages: conversationHistory,
        });

        const respuesta = res?.message?.content || res?.message || "";
        console.log("\nðŸŽ“ OrientaciÃ³n Vocacional:");
        console.log("ðŸ¤– IA:", respuesta.trim());

        console.log("\nEscribÃ­ 'salir' para terminar o contame mÃ¡s sobre ti para profundizar.");
      } catch (err) {
        console.error("âš ï¸ Error al llamar al modelo:", err);
      }
    }
  });
}

main();